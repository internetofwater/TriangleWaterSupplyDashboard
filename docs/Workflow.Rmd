---
title: "WaterSupplyDashboard_TemplateToSensorThings"
author: "Kyle Onda"
date: "6/20/2021"
output: 
  html_document:
    theme: sandstone
    highlight: zenburn
    code_folding: show
    toc: true
    number_sections: true
    toc_depth: 3
    toc_float:
      collapsed: true
      smooth_scroll: true
---

# Introduction

This document elaborates the steps necessary to ingest data from Water Supply Dashboard XLSX templates (see [blank template](https://raw.githubusercontent.com/internetofwater/TriangleWaterSupplyDashboard/master/TEMPLATE-TWSD.xlsx), and [example filled template](https://carync.box.com/shared/static/l6zlh41b3kbqxn39s9yc048s3e1u9adj.xlsx)), into an instance of the OGC SensorThings API with [Basic Authentication](https://en.wikipedia.org/wiki/Basic_access_authentication).



The following libraries are required:

```{r setup, message = FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(httr) # for HTTP requests 
library(jsonlite) # for parsing JSON from the SensorThings endpoint
library(readxl) # for reading the XLSX templates
library(dplyr) # for data manipulation
library(lubridate) # for datatime manipulation
```

And the following environmental variables are common througout the workflow:

```{r setup2}
# endpoint <- "http://web:8080/FROST-Server/v1.1/" 
# This is the production endpoint for portability
# to a different environment, assuming a docker container named "web"

endpoint <- "https://twsd.internetofwater.dev/api/v1.1/" 
#This is the current pilot endpoint 

user <- "iow" # We will be changing these in production
pw <- "nieps" # We will be changing these in production

url_registry <- "https://raw.githubusercontent.com/internetofwater/TriangleWaterSupplyDashboard/master/utility_registry.csv"
```

# Mapping the Template to SensorThings.

SensorThings API has a particular data model that we need to conceptually map the template to before ingesting data. Our first draft of this dashboard is essentially mapping 4 basic pieces of data from utilities:

1. Utility Metadata, such as its name, PWSID, location, and contact information, updated only as needed or annually. (template sheet `system_metadata`)
1. Finished water deliveries at a daily timestep, updated weekly (template sheet `delivery`)
1. Supply conditions at a daily timestep, updated weekly (generally available water storage capacity from relevant reservoirs). (template sheet `supply_conditions`)
1. Active conservation status and associated policies, updated as needed. (template sheet `conservation_status`)


SensorThings has an complex data model:

![image](https://ogc-iot.github.io/ogc-iot-api/img/SensorThingsUML_Core.svg)
We model the data like so:

* Each Utility shall be a `Thing`, identified viw the U.S. EPA SDWIS PWSID.
* Each `Thing` shall be associated with 1 `Location`, which is its Service Area Boundary
* Each `Thing` shall be associated with 2-3 `Datastream`s
* The `Datastreams` are:
  * Finished water deliveries ("[PWSID]-WaterDistributed")
    * `Sensor`: Report of delivered water ("DemandReport")
    * `ObservedProperty`: Water distributed for use by consumers of utility water ("WaterDistributed")
    * `unitOfMeasurement`: "Million Gallons per Day (MGD)"
  * Water conservation status ("[PWSID]-ConservationStatus")
    * `Sensor`: Water Shortage Status form ("StageReport")
    * `ObservedProperty`: Phase of water shortage severity associated with appropriate responses for each phase ("ConservationStatus")
    * `unitOfMeasurement`: "Status"
  * Storage Capacity ("[PWSID]-StorageCapacity")
    * `Sensor`: Water Shortage Status form ("StorageReport")
    * `ObservedProperty`: Percent of storage capacity available for distribution ("StorageCapacity")
    * `unitOfMeasurement`: "Percent"
    
# The Workflow (in Progress)

Below we describe and implement the full workflow, which is repeated for all participating utilities in a loop on a regular schedule. 
Here is the pseudocode for the whole workflow, which is elaborated more fully (only partially so far) afterwards

## Pseudocode

```{r pseudocode, eval=FALSE}
registry <- read.csv("path/to/utility_registry.csv")

PWSIDs_in_database_already <- getThings(endpoint)

PWSIDs_not_in_database_already <- registry[which(!(pwsid %in% PWSIDs_in_database_already)),]$pwsid

for(i in PWSIDs_not_in_database_already){
  data<-readExcel("path/to/excel_pwsid_i.xslx")
  postMetaDataToSTA(data,endpoint)
  createDatastreams(data,endpoint)
  postDataToSTA(data,endpoint)
}

for(i in PWSIDs_in_database_already){
  data<-readExcel("path/to/excel_pwsid_i.xslx")
  PATCH_metadata(data,endpoint)

  olddata <- getDatafromAPI(i,endpoint)
  newdata <- filter(data, datetime > max(olddata$datetime))
  postNewDatatoSTA(newdata,endpoint)

}



```

## Modular real code

1. Read in the `utility_registry.csv` [here](https://github.com/internetofwater/TriangleWaterSupplyDashboard/blob/master/utility_registry.csv) which matches a utility name and PWSID with the URL for its XLSX template.

```{r read}
registry <- read.csv(url_registry)
knitr::kable(registry)
```


1. For each row in `registry`, download the XLSX template and and upload any NEW data to the SensorThings endpoint. This will involve making HTTP POST or PATCH requests to the SensorThings endpoint. Some interactive guidance on this can be found [here](https://gost1.docs.apiary.io).
   i. If the PWSID is not already represented in the XLSX template, then the entire data model will need to be populated. To check if the PWSID is represented, we issue a GET request to the SensorThings API associated with `Thing({PWSID})`. If we receive HTTP status 200 it already exists. If we receive HTTP status 404 it does not. 
   
```{r check}
# We write a convenience function to check a
# pwsid against an endpoint variable we have already set
staThingCheck <- function(api_url){
  call <- paste0(api_url,"Things?$select=@iot.id") 
  # This makes "https://example-sta-api_url.com/endpoint-interface/Things('{id}')
  response <- jsonlite::fromJSON(call)$value$`@iot.id`
  pwsids_in_sta <- registry$pwsid[which(registry$pwsid %in% response)]
  pwsids_not_in_sta <- registry$pwsid[which(!(registry$pwsid %in% response))]
  
  pwsids <- list("pwsids_in_sta"=pwsids_in_sta,"pwsids_not_in_sta"=pwsids_not_in_sta)
  return(pwsids)
}

staThingCheck(endpoint)

```
   

   Here, we find that the Apex PWSID does not exist in SensorThings yet, so we must create it as well as its Datastreams. First we create the Thing (utility metadata object) and its associated location (service area boundary as geojson). Thus, we first must read in the entire excel template. The function below defines this, given a path (url) to a an XSLX template, we can read in the template and import a list of dataframes corresponding to the XLSX sheets into the R environment. *Note that this requires a subdirectory called `tmp` to exist in the working directory.*
   
```{r readTemplate}
readTemplate <- function(path) {
  httr::GET(path, httr::write_disk("tmp/tmp.xlsx", overwrite = TRUE))
  meta <-
    readxl::read_excel("tmp/tmp.xlsx", sheet = "system_metadata") %>% tibble::column_to_rownames(var = "field") %>% t() %>% as_tibble()
  
  sources <- readxl::read_excel("tmp/tmp.xlsx", sheet = "sources")
  monitoring_locations <-
    readxl::read_excel("tmp/tmp.xlsx", sheet = "monitoring_locations")
  conservation_policies <-
    readxl::read_excel("tmp/tmp.xlsx", sheet = "conservation_policies")
  delivery <-
    readxl::read_excel("tmp/tmp.xlsx", sheet = "delivery")
  supply <-
    readxl::read_excel("tmp/tmp.xlsx", sheet = "supply_conditions")
  monitoring_data <-
    readxl::read_excel("tmp/tmp.xlsx", sheet = "monitoring_data")
  conservation_status <-
    readxl::read_excel("tmp/tmp.xlsx", sheet = "conservation_status")
  
  unlink("tmp/tmp.xlsx")
  list <-
    list(
      metadata = meta,
      sources = sources,
      monitoring_locations = monitoring_locations,
      conservation_policies = conservation_policies,
      delivery = delivery,
      supply = supply,
      monitoring_data = monitoring_data,
      conservation_status = conservation_status
    )
  
  return(list)
}

```
   
Here we use the function to read in the XLSX associated with the first row of the utility registry (Apex in this case).
```{r downloaddata}
path <- registry$data_url[1]

data <- readTemplate(path)
head(data)
```

Now we can access the metadata sheet using `data$metadata` to create the Thing and Location into a hiearchical list object that can be POSTed as JSON to the SensorThings API

```{r makeThing, eval=FALSE}
meta <- data$metadata[1,]

metaToUtilityThing <- function(meta) {
  
  id <- paste0("NC", gsub("-", "", meta$pwsid))
  service_area <- sf::read_sf(paste0("https://geoconnex.us/ref/pws/",id))
  service_sfc <- sf::st_as_sfc(service_area)
  
  thing <-
      list(
        `@iot.id` = id,
        name = meta$system_name,
        description = paste0("Data from the water utility: ",
                             meta$system_name),
        properties = list(
          county = meta$county,
          basin = meta$basin,
          ownership = meta$ownership,
          service_population = meta$service_population,
          contact_name = paste0(meta$contact_first_name, " ", meta$contact_last_name),
          contact_title = meta$contact_title,
          address = meta$address,
          city = meta$city,
          state = meta$state,
          zip = meta$zip,
          phone = meta$phone,
          fax = meta$fax,
          email = meta$email,
          wsrp_link = meta$wsrp_link
        )
        )
  
  loc <- list(
          `@iot.id` = paste0(id," - Service Area"),
          name = paste0(id, " Service area"),
          description = paste0("Service area of ",thing$name),
          encodingType = "application/vnd.geo+json",
          location = list(
            type="Point",
            coordinates = list(
              4.9,
              52.3
            )
          )
        )
      
  utility <- list(thing=thing,location=loc)    
    
  return(utility)
}

meta <- metaToUtilityThing(meta)

```

Now we have to HTTP POST this object to STA. First we create HTTP POST, PATCH, and some convenience functions to enable this.

```{r post_utility_functions}
staPost <- function(url, payload, user, password) {
  httr::POST(
    url = url,
    encode = "json",
    httr::authenticate(user, password, type = "basic"),
    body = payload
  )
}

staPatch <- function(url, payload, user, password) {
  httr::PATCH(
    url = url,
    encode = "json",
    httr::authenticate(user, password, type = "basic"),
    body = payload
  )
}

strip_iot_id <- function(list){
  list$`@iot.id` <- NULL
  return(list)
}
```

With these convenience functions, we can now create a function that processes the entire sheet and uploads to STA.

```{r upload_utlity, eval=FALSE}


uploadUtility <- function(api, utility, user, password) { 
  id <- utility$thing$`@iot.id`
  id.l <- utility$location$`@iot.id`
  
  status <- httr::GET(paste0(api, "Things('", id, "')"))$status
  if (status == 200) {
    
    staPatch(
      url = paste0(api, "Things('", id, "')"),
      payload = strip_iot_id(utility$thing),
      user = user,
      password = password
    )
    
    staPatch(
      url = paste0(api, "Locations('", id.l, "')"),
      payload = strip_iot_id(utility$location),
      user = user,
      password = password
    )
  } else {
    staPost(
      url = paste0(api, "Things"),
      payload = utility$thing,
      user = user,
      password = password
    )
    staPost(
      url = paste0(api, "Things('", id, "')/Locations"),
      payload = utility$location,
      user = user,
      password = password
    )
  }
}

uploadUtility(endpoint, meta, user, pw)

```